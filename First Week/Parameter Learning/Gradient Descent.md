# Gradient Descent
머신러닝 9강
<pre>
J의 최솟값을 구하는 알고리즘에 대해 알아보겠다. 그것은 바로 '기울기 하강'
'기울기 하강'은 선형회귀에서만 사용되지 않고 머신러닝의 모든 부분에서 사용되고 있다.


J(θ0 , θ1)라는 함수가 있을 때 J(θ0 , θ1)의 최소화를 구하기를 원할 때

어떠한 수로 θ0 , θ1을 지정해놓고
J(θ0 , θ1)이 최소가 될 때까지 θ0, θ1의 값을 바꾸는 식으로 최소화 할 것이다.

이 함수를 최소화할려고 한다.
<img width="675" alt="스크린샷 2021-10-17 오후 5 00 42" src="https://user-images.githubusercontent.com/63940620/137617737-f225645c-3837-4d2f-b9ed-540ad0baae25.png">

최소화를 구할 때 θ0 , θ1을 어떠한 지점으로 설정한다면 아래의 그림처럼 그려질 것이다.
<img width="717" alt="스크린샷 2021-10-17 오후 5 02 30" src="https://user-images.githubusercontent.com/63940620/137617804-16b24ae7-e8cb-456c-88c6-84e196ad4c11.png">

비유를 조금 하자면, 언덕에서 가장 낮은 곳으로 이동 할 때 거리가 짧게 이동한다고 가정하자.

그러면 이러한 그림이 나온다.
<img width="698" alt="스크린샷 2021-10-17 오후 5 13 45" src="https://user-images.githubusercontent.com/63940620/137618102-c96925f9-b554-456d-b835-e113a40118df.png">

위의 이미지는 어느 한 지점에서 최솟값으로 이동하는 것이다.

하지만 시작지점이 다르다면 어떨까?


</ore>
