# Gradient Descent For Linear Regression
머신러닝 11강
<pre>

<img width="911" alt="스크린샷 2021-10-18 오후 6 14 57" src="https://user-images.githubusercontent.com/63940620/137703148-d2902795-73f0-4e95-9af5-d161017a83c2.png">
이번 시간에는 지금까지 배운 거의 모든 내용들을 활용할 것이다. (위 사진은 복습용)

함수들을 간편하게 보기 위해서 정리를 해주자.
<img width="828" alt="스크린샷 2021-10-18 오후 6 15 45" src="https://user-images.githubusercontent.com/63940620/137703291-022579e6-7846-4b34-8faa-7b305a256503.png">

여기서 θ1과 θ0은 모두 변수이다. 그러면
j가 0 일때 , j가 1일 때 이렇게 2가지로 생각할 수 있다.
이것을 위의 사진의 정리된 공식이 j가 0일 때 , j가 1일 때로 계산해보면,
아래의 식이 나온다.

<img width="881" alt="스크린샷 2021-10-18 오후 6 19 24" src="https://user-images.githubusercontent.com/63940620/137703895-e78b403e-3402-4188-97c6-93fb47ffc036.png">

여기서 이해가 되지 않는 사람이 많을 것이다. 지금의 나도 과정 다 쓰라고 하면 잘 모른다. (필자는 2021년 기준 고1이고 선행을 하지 않았다.)

<img width="679" alt="스크린샷 2021-10-18 오후 6 23 00" src="https://user-images.githubusercontent.com/63940620/137704470-2d23e7fd-ab17-4681-8761-f95e0897e597.png">
여기서 J의 값에 따른 미분계수를 계산하기 위해서는 다변수의 미적분학이 필요하다. (필자도 편미분을 사용했다는 것만 알고 있다.)

여기서 편미분이란?
  - 수많은 변수중 하나만을 변수로 , 나머지를 다 상수로 취급한 후 미분 하는 것을 의미한다.

<img width="660" alt="스크린샷 2021-10-18 오후 6 27 47" src="https://user-images.githubusercontent.com/63940620/137705233-11b3af4f-cc51-4075-a336-92fe0ef0fd0f.png">
정리한 값이다.

전의 글에서 설명한 내용 중 하나인 θ1과 θ0을 동시에 업데이트 하지 않으면 좋지 않다고 말한 것과 지역최솟값에 민감하다는 것에 대해 얘기한 적이 있다.

<img width="777" alt="스크린샷 2021-10-18 오후 6 30 28" src="https://user-images.githubusercontent.com/63940620/137705685-3faad02c-b3c4-45bd-bc8a-627e1a1723ec.png">
만약 이런 함수였다면 전역최솟값을 구하기 위해 식이 복잡해 지겠지만 

<img width="588" alt="스크린샷 2021-10-18 오후 6 34 48" src="https://user-images.githubusercontent.com/63940620/137706324-5444e5ee-3d70-4916-9e2b-710ec41bb7c1.png">
변수 2개를 사용하는 선형회귀에서는 이런 등고선 그래프 or 볼록 함수 (Convex Function)이라고 부른다.
(여기서 볼록 함수는 말그대로 볼록한 함수라는 뜻으로 간단히 기억하자.)

아무튼 볼록 함수에서 전역적 최솟값과 지역적 최솟값이 다른가?
  - 그렇지 않다. 전역적 최솟값과 지역적 최솟값이 같으므로 신경쓸 필요가 없다.

이해가 잘 안간다면 그림으로 이해하자.
<img width="904" alt="스크린샷 2021-10-18 오후 6 35 12" src="https://user-images.githubusercontent.com/63940620/137706384-358a2b68-9578-4354-a8fa-ad84256564af.png">


<img width="901" alt="스크린샷 2021-10-18 오후 6 35 40" src="https://user-images.githubusercontent.com/63940620/137706460-9ab38d2c-23c3-4acf-95aa-9766a33b9882.png">


<img width="890" alt="스크린샷 2021-10-18 오후 6 36 02" src="https://user-images.githubusercontent.com/63940620/137706529-18a35b63-9675-4c3f-a409-6cdba109b859.png">


<img width="893" alt="스크린샷 2021-10-18 오후 6 36 21" src="https://user-images.githubusercontent.com/63940620/137706570-fa74872d-6aa9-4437-a8e5-87153cccdb13.png">

등고선 그래프에서 전역적최솟값으로 이동할 수록 데이터에 점점 더 일치하는 직선을 볼 수 있다.

등고선 그래프에서의 최솟값을 가는 것이 이해가 안 갈 수 있다. 
(그럴때는 등고선 그래프에서 파란색이 낮은 부분 , 빨간색이 높은 부분이라는 것을 기억하자.)


마지막으로 이 알고리즘을 부르는 다른 이름을 배우자.
이 알고리즘을 집단 기울기 하강(Batch Gradient Descent)이라고 부른다.
'집단 기울기 하강'은 모든 기울기 하강의 단계를 확인(포함)한 것을 의미한다.

가끔 어떤 기울기 하강은 집단의 값과 다를 때도 있다. 하지만 우리는 선형회귀에서 쓰이는 기울기 하강을 공부하기 때문에 걱정할 필요가 없다.
나중에 자료의 범위 혹은 크기가 너무 거대할 때 '반복 최소 이승법'이라는 알고리즘을 사용한다. 나중에 배울 것이다.

</pre>
<pre>
한줄정리)
  - 기울기 하강을 선형회귀에 맞춰서 정리하였다.
</pre>
