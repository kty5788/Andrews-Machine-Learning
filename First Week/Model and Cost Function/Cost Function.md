# Cost Function
머신러닝 6번째
<pre>

<img width="537" alt="스크린샷 2021-10-16 오후 5 05 27" src="https://user-images.githubusercontent.com/63940620/137579793-c35e70de-02ce-48d5-9feb-0caaea979fff.png">
위의 함수는 '선형회귀'의 함수이다. 
이 문서에서는 2개의 파라미터 θ0 , θ1을 어떻게 고를 것인지에 대해 알아볼 것이다.

θ0 과 θ1 의 파라미터(변수)들의 값에 따라 '선형회귀'의 선은 달라진다.
<img width="794" alt="스크린샷 2021-10-16 오후 5 23 56" src="https://user-images.githubusercontent.com/63940620/137580375-8cc58895-1a0a-49fc-9a4b-9d909af4254c.png">

이러한 데이터가 있다고 가정해보자.
<img width="316" alt="스크린샷 2021-10-16 오후 5 25 48" src="https://user-images.githubusercontent.com/63940620/137580421-dda52340-a4b5-4be4-911b-973f72c43b1b.png">

우리는 이 데이터에서 파라미터 θ0 , θ1 들을 아래의 그림처럼 그려야 데이터들에 가장 일치하는 선형을 그을 수 있다.

<img width="295" alt="스크린샷 2021-10-16 오후 5 27 04" src="https://user-images.githubusercontent.com/63940620/137580450-beefb191-de8c-4d5b-b4de-d1c8c4668c69.png">

그러면 의문이 하나 생긴다. 어떻게 해야 θ0 , θ1이 데이터들과 잘 일치 시킬 수 있는지에 대한 궁금증이 생긴다.
여기서 데이터가 잘 일치한다는 것은 x를 통한 h(x)의 값이 y값과 최대한 가까워야 일치하는 것이다.

예를 들어 (1,1),(2,2),(3,3),(4,4),(5,5)의 데이터가 있을 때 예측을 한다면 당연히 x = y 그래프 이므로 
θ0 , θ1이 둘은 1 , 1이 되고 완전히 데이터가 일치하는 것을 볼 수 있다.

h(x)와 y 값의 데이터의 차이를 가장 최소한으로 만든다면 h(x) - y의 값이 가장 작은 상태일 것이다.

이것을 공식화 해보자.


<img width="351" alt="스크린샷 2021-10-16 오후 8 44 00" src="https://user-images.githubusercontent.com/63940620/137585995-7d5c2fdc-fd46-456f-bd17-1472030d84b5.png">

[i의 집의 입력 값에 대한 가설(hypothesis)를 예측한 것]
훈련집합에서 i = 1 부터 m(데이터 셋의 갯수)까지의 h(x(i)) - y(i) (i 번째 h(x) 값과 i 번째 y 값의 차) 의 전체 제곱 이다.

제곱을 하는 이유는?
  - 제곱을 하지 않을 때 음수가 생기면 합을 구할 때 잘못된 값을 측정하기 때문이다.

결론 :

<img width="510" alt="스크린샷 2021-10-16 오후 9 29 10" src="https://user-images.githubusercontent.com/63940620/137587442-b7f12c39-057b-4105-ae41-b244d6c05492.png">

앞에 1/2m을 하는 이유는 1/m을 함으로써 평균값을 구해 더 쉽게 연산하기 위해서이고, 1/2를 하는 것은 파라미터 θ0 , θ1을 최소화 하는 과정에서 같은 값의 과정(그니까 나중가면 할거를 지금 한다는 뜻)이라고 하는데 나도 잘 모르겠다. **** 이해 , 연구를 하게 된다면 수정하겠습니다. 이 글을 읽으시는 분들께서 알려주셔도 될 것 같아요 ****

이것처럼 최소화 혹은 최대화 시키고 싶은 선형함수를 '목적 함수'라고 한다.

최종 정리
<img width="472" alt="스크린샷 2021-10-16 오후 9 34 05" src="https://user-images.githubusercontent.com/63940620/137587609-4b21e794-d6ec-44fb-a685-ce6f1b4dec66.png">

J(θ0 , θ1) 처럼 최소화 시킨 함수를 '비용함수 (cost function)'라고 한다.
그리고 이 '비용함수'는 오차 제곱 함수(Squared error function)라고 불리기도 한다.
이 '오차 제곱 함수'는 통상적으로 사용한다.

*** 이 글을 읽으면서 이해가 되지 않아도 다음 강의부터 J 함수가 무엇인지 차근차근 알려준다고 한다. ***
</pre>
<pre>
한줄정리)
  '비용 함수'의 공식이 유도되는 과정을 배웠다. 또 왜 '비용 함수'를 사용하는지 배웠다.
</pre>
